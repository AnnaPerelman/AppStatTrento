
---
title: "Applied Statistics for High-throughput Biology: Session 7"
author: "Levi Waldron"
date: "March 24, 2016"
output:
  ioslides_presentation:
    css: styles.css
    logo: logo.png
  slidy_presentation: default
---

## Session 7 outline

- Linear models
    - Chapter 5

- Inference for high-dimensional data
    - Chapter 6

# Introduction to Linear Models

## Example: friction of spider legs

* Wolff & Gorb, [Radial arrangement of Janus-like setae permits friction control in spiders](http://www.nature.com/articles/srep01101), *Sci. Rep.* 2013.

<div class="columns-2">
<center> <img src="srep01101-f4.jpg" height=400> </center>

- **(A)** Barplot showing total claw tuft area of the corresponding legs. 
- **(B)** Boxplot presenting friction coefficient data illustrating median, interquartile range and extreme values.
</div>

## Example: friction of spider legs

<div class="columns-2">
<center> <img src="srep01101-f4.jpg" height=400> </center>

- Are the pulling and pushing friction coefficients different?
- Are the friction coefficients different for the different leg pairs?
- Does the difference between pulling and pushing friction coefficients vary by leg pair?
</div>

```{r, echo=FALSE}
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/spider_wolff_gorb_2013.csv"
filename <- "spider_wolff_gorb_2013.csv"
library(downloader)
if (!file.exists(filename))
    download(url, filename)
spider <- read.csv(filename, skip=1)
```

## Example: friction of spider legs

```{r}
table(spider$leg,spider$type)
summary(spider)
```

## Example: friction of spider legs

```{r, fig.height=4, fig.width=5, fig.align='center'}
boxplot(spider$friction ~ spider$type * spider$leg,
        col=c("grey90","grey40"), las=2,
        main="Friction coefficients of different leg pairs")
```

## Example: friction of spider legs

<div class="columns-2">

```{r, fig.height=4, fig.width=4, fig.align='center', echo=FALSE}
boxplot(spider$friction ~ spider$type * spider$leg,
        col=c("grey90","grey40"), las=2,
        main="Friction coefficients of \n different leg pairs")
```

Notes:

- Pulling friction is higher
- Pulling (but not pushing) friction increases for further back legs (L1 -> 4)
- Variance isn't constant

</div>

## What are linear models?

- Linear models model a response variable $Y_i$ as a linear combination of predictors, plus randomly distributed noise.
- Which of the following are examples of linear models?

1. $Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$
2. $Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i$
3. $Y_i = \beta_0 + \beta_1 x_i + \beta_2 \times 2^{x_i} + \varepsilon_i$

Where: $i=1,\dots,N$

Assumption: $\varepsilon_i \stackrel{iid}{\sim} N(0, \sigma_\epsilon^2)$

## What are linear models?

The following are examples of linear models:

1. $Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ (simple linear regression)
2. $Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i$ (quadratic regression)

## Multiple linear regression model

- Generally, linear models can have any number of predictors
- Systematic part of model:

$$
E[y|x] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p
$$

- $E[y|x]$ is the expected value of $y$ given $x$
- $y$ is the outcome, response, or dependent variable
- $x$ is the vector of predictors / independent variables 
- $x_p$ are the individual predictors or independent variables
- $\beta_p$ are the regression coefficients

## Multiple linear regression model

Random part of model:

$y_i = E[y_i|x_i] + \epsilon_i$

Assumptions of linear models: $\epsilon_i \stackrel{iid}{\sim} N(0, \sigma_\epsilon^2)$

* Normal distribution
* Mean zero at every value of predictors
* Constant variance at every value of predictors
* Values that are statistically independent

## Continuous predictors

* **Coding:** as-is, or may be scaled to unit variance (which results in _adjusted_ regression coefficients)
* **Interpretation for linear regression:** An increase of one unit of the predictor results in this much difference in the continuous outcome variable

## Binary predictors (2 levels)

* **Coding:** indicator or dummy variable (0-1 coding)
* **Interpretation for linear regression:** the increase or decrease in average outcome levels in the group coded “1”, compared to the reference category (“0”)
   + _e.g._ $E(y|x) = \beta_0 + \beta_1 x$ 
   + where x={ 1 if push friction, 0 if pull friction }

## Multilevel categorical predictors (ordinal or nominal)

* **Coding:** $K-1$ dummy variables for $K$-level categorical variable
* Comparisons with respect to a reference category, *e.g.* `L1`:
    * `L2`={1 if $2^{nd}$ leg pair, 0 otherwise}, 
    * `L3`={1 if $3^{nd}$ leg pair, 0 otherwise}, 
    * `L4`={1 if $4^{th}$ leg pair, 0 otherwise}.

- R re-codes factors to dummy variables automatically. 
- Dummy coding depends on whether factor is ordered or not.

## Regression in R: model formulae

[Model formulae tutorial](http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html)

* regression functions in R such as `aov()`, `lm()`, `glm()`, and `coxph()` use a "model formula" interface.
* The formula determines the model that will be built (and tested) by the R procedure. The basic format is:

`> response variable ~ explanatory variables`

* The tilde means "is modeled by" or "is modeled as a function of." 

## Model formulae (cont'd)

Model formula for simple linear regression: 

`> y ~ x`

* where "x" is the explanatory (independent) variable
* "y" is the response (dependent) variable. 

## Model formulae (cont'd)

Additional explanatory variables would be added as follows: 

`> y ~ x + z`

Note that "+" does not have its usual meaning, which would be achieved by:

`> y ~ I(x + z)`

## Interaction (effect modification)

Interaction is modeled as the product of two covariates:
$$
E[y|x] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1*x_2
$$


## Interaction (effect modification)

![Interaction between coffee and time of day on performance](coffee_interaction.jpg)
  
Image credit: http://personal.stevens.edu/~ysakamot/

## Model formulae (cont'd)

symbol  | example | meaning  
------- | ------- | --------  
+ | + x	| include this variable  
-	| - x	| delete this variable  
:	| x : z	| include the interaction  
*	| x * z	| include these variables and their interactions  
^	| (u + v + w)^3	| include these variables and all interactions up to three way
1 | -1 | intercept: delete the intercept  

## Model formulae (cont'd)

How to interpret the following model formulae?

`y ~ u + v + w + u:v + u:w + v:w`  
`y ~ u * v * w - u:v:w`  
`y ~ (u + v + w)^2`  
`> y ~ u + v + w + u:v + u:w + v:w + u:v:w`  
`> y ~ u * v * w`  
`> y ~ (u + v + w)^3`  

## Types of standard linear models

```
lm( y ~ u + v)
```
`u` and `v` factors: **ANOVA**  
`u` and `v` numeric: **multiple regression**  
one factor, one numeric: **ANCOVA**

# The Design Matrix

## The Design Matrix

Recall the multiple linear regression model:

$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{pi} + \epsilon_i$

- $x_{ji}$ is the value of predictor $x_j$ for observation $i$

## The Design Matrix

Matrix notation for the multiple linear regression model:

$$
\,
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix} = 
\begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} +
\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

or simply: 

$$
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
$$

* The design matrix is $\mathbf{X}$
    * which the computer will take as a given when solving for $\boldsymbol{\beta}$ by minimizing the sum of squares of residuals $\boldsymbol{\varepsilon}$.

## ANOVA table

Source of Variation | Sum Sq | Deg Fr | Mean Sq | F  
------------------- | ------ | -- | ------- | --  
Model | MSS | k | MSS/k | (MSS/k)/MSE  
Residual | RSS | n-(k-1) | RSS/(n-k-1) |  
Total | TSS | n-1 | |   

* $k$ = Model degrees of freedom = coefficients - 1
* $n$ = Number of observations
* **F** is F-distributed with $k$ numerator and $n-(k-1)$ denominator degrees of freedom

## Lab

- [Clustering and heatmaps exercises](http://genomicsclass.github.io/book/pages/clustering_and_heatmaps_exercises.html)
- **Batches in your own dataset**
    - Created an MDS plot colored by potential batch variables (for TCGA you have "plate" and "batch_number" variables)
    - Perform unsupervised clustering and produce a frequency table for batch vs. cluster
- Review the clustering methods used in your TCGA publication method, post to class Google Group

## Links

- A built [html][] version of this lecture is available.
- The [source][] R Markdown is also available from Github.
- A recording of the lecture will be available on the class [YouTube][] channel.

[html]: http://rpubs.com/lwaldron/TrentoSession6Lecture
[source]: https://github.com/lwaldron/AppStatTrento
[YouTube]: https://www.youtube.com/channel/UCwXiTYNRBUb_9r8-L4ziaGg