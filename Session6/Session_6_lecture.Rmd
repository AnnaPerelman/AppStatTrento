
---
title: "Applied Statistics for High-throughput Biology: Session 6"
author: "Levi Waldron"
date: "March 17, 2016"
output:
  ioslides_presentation:
    css: styles.css
    logo: logo.png
  slidy_presentation: default
---

## Session 6 outline

- Clustering
    - Book chapter 8 "Basic Machine Learning", Clustering section
- Batch Effects
    - Book chapter 9 "Batch Effects"

## Review: distances

- Recall that metrics have strict requirements
- Distances have fewer requirements
- Similarity and Dissimilarity have fewer yet:

    1. non-negativity $d(a, b) \ge 0$
    2. symmetry $d(a, b) = d(b, a)$
    3. Increase or decrease for more "similar" vectors, by whatever definition

## Tissue gene expression dataset

Recall the tissue gene expression dataset:
```{r}
##biocLite("genomicsclass/tissuesGeneExpression")
library(tissuesGeneExpression)
data(tissuesGeneExpression)
dim(e) ##gene expression data
table(tissue) ##tissue[i] corresponds to e[,i]
```

## Sample pairwise Euclidian distance

```{r, cache=TRUE}
d.euclidian <- dist( t(e) )
```

```{r, echo=FALSE, fig.height=4.5, fig.width=4.5, fig.align='center'}
pheatmap::pheatmap(d.euclidian)
```

## Sample pairwise 1-Pearson Correlation

```{r, cache=TRUE}
d.pearson <- as.dist( 1 - cor(e) )
```

```{r, echo=FALSE, fig.height=4.5, fig.width=4.5, fig.align='center'}
pheatmap::pheatmap(d.pearson)
```

# Hierarchical clustering

## Hierarchical clustering

- Clustering algorithms use dissimilarity between observations to form similar groups
- Hierarchical clustering is one of many such algorithms
- **Agglomerative (bottom-up)** - what we will use:
    - Start with each point in own group
    - Repeatedly merge the two groups with smallest dissimilarity, until one cluster remains
- **Divisive (top-down)**:
    - Start with all points in a single group
    - Repeatedly split the group into two resulting in the biggest dissimilarity, until each point is in own group
- ?hclust includes detailed information

## Hierarchical clustering

```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
set.seed(1)
mat <- cbind(c(0, 0, 0, 1, 1, 1) + rnorm(6), rnorm(6))
rownames(mat) <- rank(mat[, 1])
rafalib::mypar(mfrow=c(1, 2))
plot(mat[, 1], mat[, 2], type="n", xlab="x", ylab="y", asp=1)
text(mat[, 1], mat[, 2], labels=rownames(mat))
plot(as.dendrogram(hclust(dist(mat))))
```

** Using default `hclust()` settings (**"complete"** linkage)

## Hierarchical clustering

```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
rafalib::mypar(mfrow=c(1, 2))
plot(mat[, 1], mat[, 2], type="n", xlab="x", ylab="y", asp=1)
text(mat[, 1], mat[, 2], labels=rownames(mat))
plot(as.dendrogram(hclust(dist(mat), method="single")))
```

** **"single"** linkage (uncommon)

## Hierarchical clustering

Note that many equivalent re-orderings exist:

```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
rafalib::mypar(mfrow=c(1, 3))
dend <- as.dendrogram(hclust(dist(mat)))
plot(dend)
dend2 <- reorder(dend, wts=mat[, 1])
plot(dend2)
dend3 <- reorder(dend, wts=-mat[, 1])
plot(dend3)
```

- See `?reorder.dendrogram`

## Hierarchical clustering on tissues data

```{r color_dendrogram,fig.width=8,fig.height=4.5, fig.align='center', echo=FALSE}
rafalib::mypar(mar=c(0, 2.5, 1.6, 1.1))
hc.euclidian <- hclust(d.euclidian)
rafalib::myplclust(hc.euclidian, labels=tissue, lab.col=as.numeric(factor(tissue)), 
                   main="Euclidian Distance Cluster Dendrogram", cex=0.5)
legend("topright", legend=levels(factor(tissue)), lty=1, lw=3,
       col=1:length(unique(tissue)), bty="n")
```

- `hclust()` default, Euclidian distance

## Hierarchical clustering on tissues data

```{r color_dendrogram2,fig.width=8,fig.height=4.5, fig.align='center', echo=FALSE}
rafalib::mypar(mar=c(0, 2.5, 1.6, 1.1))
hc.pearson <- hclust(d.pearson)
rafalib::myplclust(hc.pearson, labels=tissue, lab.col=as.numeric(factor(tissue)), 
                   main="1 - PCC Distance Cluster Dendrogram", cex=0.5)
legend("topright", legend=levels(factor(tissue)), lty=1,  lw=3,
       col=1:length(unique(tissue)), bty="n")
```

- `hclust()` default, 1 - Pearson Correlation distance

## Generating actual clusters

- Hierarchical clustering produces a **dendrogram**
- To get clusters, you must "cut" the tree

```{r color_dendrogram3,fig.width=8,fig.height=4, fig.align='center', echo=FALSE}
rafalib::mypar(mar=c(0, 2.5, 1.6, 1.1))
rafalib::myplclust(hc.pearson, labels=tissue, lab.col=as.numeric(factor(tissue)), 
                   main="1 - PCC Distance Cluster Dendrogram", cex=0.5)
legend("topright", legend=levels(factor(tissue)), lty=1,  lw=3,
       col=1:length(unique(tissue)), bty="n")
abline(h=0.125, lw=3, lty=2)
```

## Generating actual clusters

Using the above cutoff, how do clusters overlap with the actual tissues?

```{r}
hclusters <- cutree(hc.pearson, h=0.125)
table(true=tissue, cluster=hclusters)
```

## Generating actual clusters

Alternatively, specifying the number of clusters:

```{r}
hclusters <- cutree(hc.pearson, k=length(unique(tissue)) + 1 )
table(true=tissue, cluster=hclusters)
```

# K-means clustering

## K-means clustering

- K-means clustering iteratively finds K clusters to minimize total within-cluster distance
    - often (but not necessarily) using Euclidian distance

```{r, echo=FALSE}
hclusters.euclidian <- cutree(hc.euclidian, k=8)
```

    
```{r, cache=TRUE}
set.seed(1); km <- kmeans(t(e), centers=8)
table(Kmeans=km$cluster, Hclust=hclusters.euclidian)
```

## How many clusters?

- Common approaches for estimating the number of clusters are:
    - Prediction Strength, Silhouette Width, Gap Statistic
    - See `library(cluster)`
    - A demonstration of all of these, combined with different dissimilarity measures for microbiome communities, in Koren *et al.* [1]

<font size="2">
[1] Koren, O. et al. [A guide to enterotypes across the human body: meta-analysis of microbial community structures in human microbiome datasets.](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002863) PLoS Comput. Biol. 9, e1002863 (2013).
</font>

## Recall the good old clustered heatmap

```{r, message=FALSE, echo=FALSE, fig.align='center'}
library(pheatmap)
rv <- genefilter::rowVars(e)
idx <- order(-rv)[1:40]
annot <- data.frame(tissue=factor(tissue),
                    brain=factor(tissue %in% c("cerebellum", "hippocampus")))
rownames(annot) <- colnames(e)
ann_colors <- list(tissue = RColorBrewer::brewer.pal(n=7, "Set1"), 
                   brain=c("black", "grey"))
names(ann_colors[[1]]) <- unique(tissue)
names(ann_colors[[2]]) <- c("TRUE", "FALSE")
pheatmap::pheatmap(e[idx, ], show_colnames = FALSE, 
                   annotation_colors = ann_colors, annotation_col = annot)
```

**Exercises**: Look at `?pheatmap::pheatmap`, try `example("pheatmap::pheatmap")`

## Summary: clustering

- There are many algorithms, e.g.:
    - Partitioning around Medoids (PAM, see `?cluster::pam`, `?pamr::pamr.cv`)
    - Non-negative Matrix Factorization (NMF, see `?NMF::nmf`)
- **Remember** that the choice of distance function is a part of the clustering algorithm
- Unsupervised clustering provides a quantitative approach to a fundamentally qualitative problem

# Batch Effects

## Batch Effects

- pervasive in genomics (e.g. [Leek *et al.*](https://www.ncbi.nlm.nih.gov/pubmed/?term=20838408) )
- affect DNA and RNA sequencing, proteomics, imaging, microarray...
- have caused high-profile problems and retractions
    - you can't get rid of them
    - but you can make sure they are not confounded with your experiment
    
## Batch Effects - an example

- Nat Genet. 2007 Feb;39(2):226-31. Epub 2007 Jan 7.
- Title: *Common genetic variants account for differences in gene expression among ethnic groups.*
    - "The results show that specific genetic variation among populations contributes appreciably to differences in gene expression phenotypes."

```{r, message=FALSE}
library(Biobase)
library(genefilter)
library(GSE5859) ## BiocInstaller::biocLite("genomicsclass/GSE5859")
data(GSE5859)
geneExpression = exprs(e)
sampleInfo = pData(e)
```

## Batch Effects - an example

- Sample metadata included *date of processing*:

```{r}
head(table(sampleInfo$date))
```

## Batch Effects - an example

```{r}
year <-  as.integer(format(sampleInfo$date,"%y") )
year <- year - min(year)
month = as.integer( format(sampleInfo$date,"%m") ) + 12*year
table(year, sampleInfo$ethnicity)
```

## Batch Effects: PCA

```{r, cache=TRUE, warning=FALSE}
pc <- princomp(geneExpression, cor=TRUE)
boxplot(pc$loadings[, 2] ~ month, varwidth=TRUE, notch=TRUE,
        main="PC2 vs. month", xlab="Month", ylab="PC2")
```

## Batch Effects: MDS

A starting point for a color palette:
```{r, eval=FALSE}
RColorBrewer::display.brewer.all(n=3, colorblindFriendly = TRUE)
```

Interpolate one color per month on a quantitative palette:
```{r}
col3 <- c(RColorBrewer::brewer.pal(n=3, "Greys")[2:3], "black")
MYcols <- colorRampPalette(col3, space="Lab")(length(unique(month)))
```

## Batch Effects: MDS

```{r, fig.height=3.5, fig.align='center'}
d <- as.dist(1 - cor(geneExpression))
mds <- cmdscale(d)
plot(mds, col=MYcols[as.integer(factor(month))],
     main="MDS shaded by batch")
```

## Batch Effects: clustering

```{r}
hcclass <- cutree(hclust(d), k=5)
table(hcclass, year)
```

## Batch Effects - summary

- tend to affect many or all measurements by a little bit
- During experimental design:
    - keep track of anything that might cause a batch effect for post-hoc analysis
    - include control samples in each batch
- batches can be corrected for if randomized in study design
    - if confounded with treatment or outcome of interest, nothing can help you

## Lab

- [Clustering and heatmaps exercises](http://genomicsclass.github.io/book/pages/clustering_and_heatmaps_exercises.html)
- **Batches in your own dataset**
    - Created an MDS plot colored by potential batch variables (for TCGA you have "plate" and "batch_number" variables)
    - Perform unsupervised clustering and produce a frequency table for batch vs. cluster
- Review the clustering methods used in your TCGA publication method, post to class Google Group

## Links

- A built [html][] version of this lecture is available.
- The [source][] R Markdown is also available from Github.
- A recording of the lecture will be available on the class [YouTube][] channel.

[html]: http://rpubs.com/lwaldron/TrentoSession6Lecture
[source]: https://github.com/lwaldron/AppStatTrento
[YouTube]: https://www.youtube.com/channel/UCwXiTYNRBUb_9r8-L4ziaGg