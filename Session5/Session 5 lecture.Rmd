
---
title: "Applied Statistics for High-throughput Biology: Session 5"
author: "Levi Waldron"
date: "March 10, 2016"
output:
  ioslides_presentation:
    css: styles.css
    logo: logo.png
  slidy_presentation: default
---

## Session 5 outline

- Distances in high dimensions
- Principal Components Analysis
- Multidimensional Scaling

Book chapter 7

# Distances in high-dimensional data analysis

## The importance of distance

- High-dimensional data are complex and impossible to visualize in raw form
- Thousands of dimensions, we can only visualize 2-3
- Distances can simplify thousands of dimensions

<center>
<img src="animals.png" alt="animals" align="middle" style="height: 350px;">
</center>

## The importance of distance (cont'd)

- Distances can help organize samples and genomic features

```{r, echo=FALSE}
library(GSE5859Subset)
data(GSE5859Subset) ##this loads three tables
set.seed(1)
ge = geneExpression[sample(1:nrow(geneExpression), 200), ]
pheatmap::pheatmap(ge, scale="row", show_colnames = F, show_rownames = F)
```

## The importance of distance (cont'd)

- Any clustering or classification of samples and/or genes involves
combining or identifying objects that are close or similar.
- Distances or similarities are mathematical representations of what
we mean by close or similar.
- The choice of distance is important and requires thought. 
    - choice is subject-matter specific

<font size="2">
Source: http://master.bioconductor.org/help/course-materials/2002/Summer02Course/Distance/distance.pdf
</font>

## Metrics and distances

A **metric** satisfies the following five properties:

1. non-negativity $d(a, b) \ge 0$
2. symmetry $d(a, b) = d(b, a)$
3. identification mark $d(a, a) = 0$
4. definiteness $d(a, b) = 0$ if and only if $a=b$
5. triangle inequality $d(a, b) + d(b, c) \ge d(a, c)$

- A **distance** is only required to satisfy 1-3.
- A **similarity function** satisfies 1-2, and **increases** as $a$ and $b$ become more similar
- A **dissimilarity function** satisfies 1-2, and **decreases** as $a$ and $b$ become more similar


## Euclidian distance (metric)

- Remember grade school:
```{r, echo=FALSE, fig.height=3.5}
rafalib::mypar()
plot(c(0,1,1),c(0,0,1),pch=16,cex=2,xaxt="n",yaxt="n",xlab="",ylab="",bty="n",xlim=c(-0.25,1.25),ylim=c(-0.25,1.25))
lines(c(0,1,1,0),c(0,0,1,0))
text(0,.2,expression(paste('(A'[x]*',A'[y]*')')),cex=1.5)
text(1,1.2,expression(paste('(B'[x]*',B'[y]*')')),cex=1.5)
text(-0.1,0,"A",cex=2)
text(1.1,1,"B",cex=2)
```
<center>
Euclidean d = $\sqrt{ (A_x-B_x)^2 + (A_y-B_y)^2}$.
</center>

- **Side note**: also referred to as *$L_2$ norm*

## Euclidian distance in high dimensions

```{r}
##biocLite("genomicsclass/tissuesGeneExpression")
library(tissuesGeneExpression)
data(tissuesGeneExpression)
dim(e) ##gene expression data
table(tissue) ##tissue[i] corresponds to e[,i]
```

Interested in identifying similar *samples* and similar *genes*

## Euclidian distance in high dimensions

- Points are no longer on the Cartesian plane,
- instead they are in higher dimensions. For example:
    - sample $i$ is defined by a point in 22,215 dimensional space: $(Y_{1,i},\dots,Y_{22215,i})^\top$. 
    - feature $g$ is defined by a point in 189 dimensions $(Y_{g,189},\dots,Y_{g,189})^\top$

## Euclidian distance in high dimensions

Euclidean distance as for two dimensions. E.g., the distance between two samples $i$ and $j$ is:

$$ \mbox{dist}(i,j) = \sqrt{ \sum_{g=1}^{22215} (Y_{g,i}-Y_{g,j })^2 } $$

and the distance between two features $h$ and $g$ is:

$$ \mbox{dist}(h,g) = \sqrt{ \sum_{i=1}^{189} (Y_{h,i}-Y_{g,i})^2 } $$

## Matrix algebra notation

The distance between samples $i$ and $j$ can be written as:

$$ \mbox{dist}(i,j) = \sqrt{ (\mathbf{Y}_i - \mathbf{Y}_j)^\top(\mathbf{Y}_i - \mathbf{Y}_j) }$$

with $\mathbf{Y}_i$ and $\mathbf{Y}_j$ columns $i$ and $j$. 

## Matrix algebra notation

```{r}
t(matrix(1:3, ncol=1))
matrix(1:3, ncol=1)
t(matrix(1:3, ncol=1)) %*% matrix(1:3, ncol=1)
```

Note: R is very efficient at matrix algebra

## 3 sample example

```{r}
kidney1 <- e[, 1]
kidney2 <- e[, 2]
colon1 <- e[, 87]
sqrt(sum((kidney1 - kidney2)^2))
sqrt(sum((kidney1 - colon1)^2))
```

## 3 sample example using dist()

```{r}
dim(e)
(d <- dist(t(e[, c(1, 2, 87)])))
class(d)
```

## The dist() function

Excerpt from ?dist:

```{r, eval=FALSE}
dist(x, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)
```

- **method:** the distance measure to be used. 
    - This must be one of "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski". Any unambiguous substring can be given.
- `dist` class output from `dist()` is used for many clustering algorithms and heatmap functions

*Caution*: `dist(e)` creates a `r nrow(e)` x `r nrow(e)` matrix that will probably crash your R session.

## Note on standardization

- In practice, features (e.g. genes) are typically "standardized", *i.e.* converted to z-score:

$$x_{gi} \leftarrow \frac{(x_{gi} - \bar{x}_g)}{s_g}$$

- This is done because the differences in overall levels between features are often not due to biological effects but technical ones, *e.g.*:
    - GC bias, PCR amplification efficiency, ...
- Euclidian distance and $1-r$ (Pearson cor) are related:
    - $\frac{d_E(x, y)^2}{2m} = 1 - r_{xy}$

**Distance Exercises**: p. 322-323

# Dimension reduction and PCA

## Motivation for dimension reduction

Simulate the heights of twin pairs:
```{r}
library(MASS)
set.seed(1)
n <- 100
y <- t(MASS::mvrnorm(n, c(0,0), matrix(c(1, 0.95, 0.95, 1), 2, 2)))
dim(y)
cor(t(y))
```

## Motivation for dimension reduction

```{r, echo=FALSE}
z1 = (y[1,]+y[2,])/2 #the sum 
z2 = (y[1,]-y[2,])   #the difference

z = rbind( z1, z2) #matrix now same dimensions as y

thelim <- c(-3,3)
rafalib::mypar(1,2)

plot(y[1,],y[2,],xlab="Twin 1 (standardized height)",ylab="Twin 2 (standardized height)",xlim=thelim,ylim=thelim,
     main="Original twin heights")
points(y[1,1:2],y[2,1:2],col=2,pch=16)

plot(z[1,],z[2,],xlim=thelim,ylim=thelim,xlab="Average height",ylab="Differnece in height",
     main="Principal Components projection")
points(z[1,1:2],z[2,1:2],col=2,pch=16)
```

## Motivation for dimension reduction

<center>
```{r, echo=FALSE, fig.height=3.5, fig.width=3.5}
rafalib::mypar()
d = dist(t(y))
d3 = dist(z[1,]) * sqrt(2) ##distance computed using just first dimension mypar(1,1)
plot(as.numeric(d), as.numeric(d3), xlab="Pairwise distances in 2 dimensions", 
     ylab="Pairwise distances in 1 dimension")
abline(0,1, col="red")
```
</center>

- Not much loss of height differences when just using average heights of twin pairs.
    - because twin heights are highly correlated

## Singular Value Decomposition (SVD)

SVD generalizes the example rotation we looked at:

$$\mathbf{Y} = \mathbf{UDV}^\top$$

<center>
<img src="SVD1.png" alt="SVD" align="middle" style="height: 350px;">
</center>

- **note**: the above formulation is for $m > n$

## Singular Value Decomposition (SVD)

<center>
<img src="SVD1.png" alt="SVD" align="middle" style="height: 275px;">
</center>

- $\mathbf{Y}$: the m rows x n cols matrix of measurements
- $\mathbf{U}$: m x n *orthogonal* matrix (**scores**)
    - orthogonal = unit length and "perpendicular" in 3-D
- $\mathbf{D}$: n x n diagonal matrix (**eigenvalues**)
- $\mathbf{V}$: n Ã— n orthogonal matrix (**eigenvectors or loadings**)

## SVD of gene expression dataset

```{r, cache=TRUE}
system.time(e.standardize.slow <- t(apply(e, 1, function(x) x - mean(x) )))
system.time(e.standardize.fast <- t(scale(t(e), scale=FALSE)))
## all.equal(e.standardize.slow[, 1], e.standardize.fast[, 1])
s <- svd(e.standardize.fast)
names(s)
```

## SVD of gene expression dataset

```{r}
dim(s$u)     # loadings
length(s$d)  # eigenvalues
dim(s$v)     # d %*% vT = scores
```
<center>
<img src="SVD1.png" alt="SVD" align="middle" style="height: 200px;">
</center>

## PCA of gene expression dataset

```{r, cache=TRUE}
p <- princomp(e.standardize.fast)  #cor=TRUE to scale rowss
```

```{r, fig.height=3, fig.width=3, fig.align='center', echo=FALSE}
rafalib::mypar()
plot((s$d^2 / sum(s$d^2) * 100), (p$sdev^2 / sum(p$sdev^2) * 100),
     xlab="% variance explained (SVD)", ylab="% variance explained (PCA)")
```


## PCA interpretation: scores

<center>
<img src="SVD1.png" alt="SVD" align="middle" style="height: 225px;">
</center>

- $\mathbf{U}$ (**scores**): relate the *principal component* axes to the original variables
    - think of principal component axes as a weighted combination of original axes

## PCA interpretation: scores

```{r}
plot(p$scores[, 1], xlab="Index of genes", ylab="Scores of PC1", 
     main="Scores of PC1") #or, predict(p)
abline(h=c(-35, 25), col="red")
```

## PCA interpretation: scores

Genes with high PC1 scores:

```{r, fig.height=3.5}
e.pc1genes <- e[which(p$scores[, 1] < -35 | p$scores[, 1] > 25), ]
pheatmap::pheatmap(e.pc1genes, scale="none", show_rownames=FALSE, 
                  show_colnames = FALSE)
```


## PCA interpretation: eigenvalues

- $\mathbf{D}$ (**eigenvalues**): standard deviation scaling factor that each decomposed variable is multiplied by.
```{r, eval=FALSE}
plot(p$sdev^2 / sum(p$sdev^2)*100, ylab="% variance explained")
```
```{r, fig.height=3, fig.width=5, echo=FALSE, fig.align='center'}
rafalib::mypar()
plot(p$sdev^2 / sum(p$sdev^2)*100, ylab="% variance explained", main="Screeplot")
```

## PCA interpretation: eigenvalues

Alternatively as cumulative % variance explained (using `cumsum()` function):
```{r, fig.height=4, echo=FALSE, fig.align='center'}
rafalib::mypar()
plot(cumsum(p$sdev^2)/sum(p$sdev^2)*100, 
  ylab="cumulative % variance explained", ylim=c(0, 100), 
  type="l", main="Cumulative screeplot")
```

## PCA interpretation: loadings

<center>
<img src="SVD1.png" alt="SVD" align="middle" style="height: 225px;">
</center>

- $\mathbf{V}$ (**loadings**): The "datapoints" in the reduced prinipal component space

## PCA interpretation: loadings

```{r, fig.height=5, echo=FALSE}
rafalib::mypar()
plot(p$loadings[, 1:2], xlab="PC1", ylab="PC2", 
     main="plot of p$loadings[, 1:2]",
     col=factor(tissue), pch=as.integer(factor(tissue)))
legend("topleft", legend=levels(factor(tissue)), col=1:length(unique(tissue)),
       pch=1:length(unique(tissue)), bty='n')
```

## PCA Lab

1. Re-create PC plot with per-gene scaling
    a) by explicitly scaling the rows, and
    b) by setting `cor=TRUE` in the `princomp()` function
2. Re-create screeplot using built-in plot function

## Multi-dimensional Scaling (MDS)

- also referred to as Principal Coordinates Analysis (PCoA)
- a reduced SVD, performed on a distance matrix
- identify two (or more) eigenvalues/vectors that preserve distances

```{r}
d <- as.dist(1 - cor(e.standardize.fast))
mds <- cmdscale(d)
```

## Multi-dimensional Scaling (MDS)

```{r, echo=FALSE, fig.height=5}
rafalib::mypar()
plot(mds, col=factor(tissue), pch=as.integer(factor(tissue)))
legend("topright", legend=levels(factor(tissue)), col=1:length(unique(tissue)),
       bty='n', pch=1:length(unique(tissue)))
```

## Conclusions

- **Note**: signs of eigenvalues (square to get variances) and eigenvectors (loadings) can be arbitrarily flipped
- PCA and MDS are useful for dimension reduction when you have **correlated variables**
- Variables are always centered.  
- Variables are also scaled unless you know they have the same scale in the population
- PCA projection can be applied to new datasets if you know the matrix calculations
- PCA is subject to over-fitting, screeplot can be tested by cross-validation

## Lab

- [Distance Exercises](http://genomicsclass.github.io/book/pages/distance_exercises.html)
- [MDS exercises](http://genomicsclass.github.io/book/pages/mds_exercises.html)

## Links

- A built [html][] version of this lecture is available.
- The [source][] R Markdown is also available from Github.
- A recording of the lecture will be available on the class [YouTube][] channel.

[html]: http://rpubs.com/lwaldron/TrentoSession5Lecture
[source]: https://github.com/lwaldron/AppStatTrento
[YouTube]: https://www.youtube.com/channel/UCwXiTYNRBUb_9r8-L4ziaGg